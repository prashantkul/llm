{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training notebook for PyTorch based transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.nn.functional import log_softmax\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "\n",
    "from training.dataset import create_datasets_and_loaders\n",
    "from training.dateformattransformer import DateFormatTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(outputs, targets, vocab):\n",
    "    # Assuming outputs are logits\n",
    "    predictions = outputs.argmax(dim=-1)\n",
    "    \n",
    "    # Adjust predictions or targets to match in length\n",
    "    min_len = min(predictions.size(1), targets.size(1))\n",
    "    predictions = predictions[:, :min_len]\n",
    "    targets = targets[:, :min_len]\n",
    "    \n",
    "    correct = (predictions == targets).float()\n",
    "    \n",
    "    # Mask out padding tokens\n",
    "    mask = (targets != vocab['<PAD>']).float()\n",
    "    accuracy = (correct * mask).sum() / mask.sum()\n",
    "    \n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training function\n",
    "\n",
    "This function will be used to train the model. The function uses a technique called Curriculum Learning, in which the model is trained on a small dataset first, and then on a larger dataset. This is done to ensure that the model is not overfitting to the small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_curriculum(model, dataloaders, num_epochs, device, criterion, optimizer, patience, vocab):\n",
    "    difficulties = ['easy']\n",
    "    \n",
    "    for difficulty in difficulties:\n",
    "        print(f\"Training on {difficulty} dataset\")\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_without_improvement = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            total_train_loss = 0\n",
    "            total_train_accuracy = 0\n",
    "            num_train_batches = 0\n",
    "            \n",
    "            for batch in dataloaders[difficulty]['train']:\n",
    "                inputs = batch['input'].to(device)\n",
    "                targets = batch['output'].to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(inputs, targets[:, :-1])  # The model will create masks internally\n",
    "                loss = criterion(output.contiguous().view(-1, len(vocab)), targets[:, 1:].contiguous().view(-1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_train_loss += loss.item()\n",
    "                total_train_accuracy += calculate_accuracy(output, targets[:, 1:], vocab)\n",
    "                num_train_batches += 1\n",
    "            \n",
    "            avg_train_loss = total_train_loss / num_train_batches\n",
    "            avg_train_accuracy = total_train_accuracy / num_train_batches\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            total_val_accuracy = 0\n",
    "            num_val_batches = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in dataloaders[difficulty]['val']:\n",
    "                    inputs = batch['input'].to(device)\n",
    "                    targets = batch['output'].to(device)\n",
    "                    \n",
    "                    output = model(inputs, targets[:, :-1])\n",
    "                    loss = criterion(output.contiguous().view(-1, len(vocab)), targets[:, 1:].contiguous().view(-1))\n",
    "                    \n",
    "                    total_val_loss += loss.item()\n",
    "                    total_val_accuracy += calculate_accuracy(output, targets[:, 1:], vocab)\n",
    "                    num_val_batches += 1\n",
    "            \n",
    "            avg_val_loss = total_val_loss / num_val_batches\n",
    "            avg_val_accuracy = total_val_accuracy / num_val_batches\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_accuracy:.4f}\")\n",
    "            print(f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\n",
    "            \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_without_improvement = 0\n",
    "                torch.save(model.state_dict(), f'best_model_{difficulty}.pth')\n",
    "                print(\"Saved new best model\")\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "            \n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "            \n",
    "            print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to inspect PyTorch datasets and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_dataloader_data(dataloader, dataset_dict):\n",
    "    print(\"Inspecting data...\")\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['input']\n",
    "        targets = batch['output']\n",
    "        \n",
    "        print(\"\\nSample batch:\")\n",
    "        original_dataset = dataset_dict['train'].dataset\n",
    "        for i in range(min(5, inputs.size(0))):\n",
    "            try:\n",
    "                input_date = original_dataset.indices_to_date(inputs[i])\n",
    "                target_date = original_dataset.indices_to_date(targets[i])\n",
    "                print(f\"Input: {input_date} | Target: {target_date}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {i}: {str(e)}\")\n",
    "                print(f\"Input indices: {inputs[i]}\")\n",
    "                print(f\"Target indices: {targets[i]}\")\n",
    "        \n",
    "        # Only process one batch\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_indices(date_string, vocab, max_length):\n",
    "    \"\"\" Convert a date string to a sequence of indices. Indics are padded to max_length. \"\"\"\n",
    "    tokens = [vocab.get(char, vocab['<UNK>']) for char in date_string]\n",
    "    tokens = [vocab['<START>']] + tokens + [vocab['<END>']]\n",
    "    padding = [vocab['<PAD>']] * (max_length - len(tokens))\n",
    "    return tokens + padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_dataloader, device, vocab):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    idx_to_char = {idx: char for char, idx in vocab.items()}\n",
    "    \n",
    "    print(\"Testing model...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader):\n",
    "            inputs = batch['input'].to(device)\n",
    "            targets = batch['output'].to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, dim=-1)\n",
    "            \n",
    "            # Compare predictions with targets\n",
    "            mask = (targets != vocab['<PAD>']) & (targets != vocab['<START>']) & (targets != vocab['<END>'])\n",
    "            correct += ((predicted == targets) & mask).sum().item()\n",
    "            total += mask.sum().item()\n",
    "            \n",
    "            # Print some examples\n",
    "            if total % 1000 == 0:\n",
    "                print(\"\\nExample conversions:\")\n",
    "                for i in range(min(3, inputs.size(0))):\n",
    "                    input_date = ''.join([idx_to_char[idx.item()] for idx in inputs[i] if idx.item() not in [vocab['<PAD>'], vocab['<START>'], vocab['<END>']]])\n",
    "                    target_date = ''.join([idx_to_char[idx.item()] for idx in targets[i] if idx.item() not in [vocab['<PAD>'], vocab['<START>'], vocab['<END>']]])\n",
    "                    pred_date = ''.join([idx_to_char[idx.item()] for idx in predicted[i] if idx.item() not in [vocab['<PAD>'], vocab['<START>'], vocab['<END>']]])\n",
    "                    print(f\"Input: {input_date} | Target: {target_date} | Predicted: {pred_date}\")\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to generate train, val and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_date_dataset(samples: int):\n",
    "    num_samples = 50000\n",
    "    batch_size = 32\n",
    "\n",
    "    # Generate datasets and create dataloaders\n",
    "    datasets, dataloaders, vocab = create_datasets_and_loaders(num_samples, batch_size)\n",
    "\n",
    "    print(f\"Shared vocabulary size: {len(vocab)}\")\n",
    "    print(\"Printing a sample from each dataset..\")\n",
    "    for difficulty in ['easy', 'medium', 'hard']:\n",
    "        print(f\"\\nInspecting {difficulty} dataset:\")\n",
    "        inspect_dataloader_data(dataloaders[difficulty]['train'], datasets[difficulty])\n",
    "    \n",
    "    return datasets, dataloaders, vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Mode: training\n",
      "\n",
      "Generating easy dataset:\n",
      "Raw data samples:\n",
      "Input: 10/14/1957 | Output: 10/14/1957\n",
      "Input: 2093-06-17 | Output: 06/17/2093\n",
      "Input: 26/01/1914 | Output: 01/26/1914\n",
      "Input: 2045-05-25 | Output: 05/25/2045\n",
      "Input: 04/04/2038 | Output: 04/04/2038\n",
      "\n",
      "Generating medium dataset:\n",
      "Raw data samples:\n",
      "Input: 10/06/2055 | Output: 10/06/2055\n",
      "Input: 1949-11-25 | Output: 11/25/1949\n",
      "Input: 26/05/2024 | Output: 05/26/2024\n",
      "Input: May 22, 2050 | Output: 05/22/2050\n",
      "Input: February 14, 2006 | Output: 02/14/2006\n",
      "\n",
      "Generating hard dataset:\n",
      "Raw data samples:\n",
      "Input: October 20, 1921 | Output: 10/20/1921\n",
      "Input: 2068/12/22 | Output: 12/22/2068\n",
      "Input: 06/11/1967 | Output: 06/11/1967\n",
      "Input: 2056/10/04 | Output: 10/04/2056\n",
      "Input: 1957.07.04 | Output: 07/04/1957\n",
      "Shared vocabulary size: 45\n",
      "Printing a sample from each dataset..\n",
      "\n",
      "Inspecting easy dataset:\n",
      "Inspecting data...\n",
      "\n",
      "Sample batch:\n",
      "Input: 09/10/2069 | Target: 09/10/2069\n",
      "Input: 03/01/1989 | Target: 03/01/1989\n",
      "Input: 08/11/1933 | Target: 08/11/1933\n",
      "Input: 10/04/1943 | Target: 04/10/1943\n",
      "Input: 1985-02-18 | Target: 02/18/1985\n",
      "\n",
      "Inspecting medium dataset:\n",
      "Inspecting data...\n",
      "\n",
      "Sample batch:\n",
      "Input: 19/09/2002 | Target: 09/19/2002\n",
      "Input: 1916-02-06 | Target: 02/06/1916\n",
      "Input: 11/23/1989 | Target: 11/23/1989\n",
      "Input: 09/07/1919 | Target: 09/07/1919\n",
      "Input: December 08, 2036 | Target: 12/08/2036\n",
      "\n",
      "Inspecting hard dataset:\n",
      "Inspecting data...\n",
      "\n",
      "Sample batch:\n",
      "Input: 12.25.1979 | Target: 12/25/1979\n",
      "Input: 05.09.2074 | Target: 05/09/2074\n",
      "Input: 31.03.2043 | Target: 03/31/2043\n",
      "Input: 14/01/2057 | Target: 01/14/2057\n",
      "Input: 26.08.2081 | Target: 08/26/2081\n",
      "Training on easy dataset\n",
      "Epoch 1/50\n",
      "Train Loss: 0.3172, Train Accuracy: 0.8873\n",
      "Val Loss: 0.0331, Val Accuracy: 0.9854\n",
      "Saved new best model\n",
      "--------------------------------------------------\n",
      "Epoch 2/50\n",
      "Train Loss: 0.0328, Train Accuracy: 0.9857\n",
      "Val Loss: 0.0188, Val Accuracy: 0.9886\n",
      "Saved new best model\n",
      "--------------------------------------------------\n",
      "Epoch 3/50\n",
      "Train Loss: 0.0230, Train Accuracy: 0.9882\n",
      "Val Loss: 0.0181, Val Accuracy: 0.9888\n",
      "Saved new best model\n",
      "--------------------------------------------------\n",
      "Epoch 4/50\n",
      "Train Loss: 0.0215, Train Accuracy: 0.9884\n",
      "Val Loss: 0.0178, Val Accuracy: 0.9888\n",
      "Saved new best model\n",
      "--------------------------------------------------\n",
      "Epoch 5/50\n",
      "Train Loss: 0.0210, Train Accuracy: 0.9884\n",
      "Val Loss: 0.0188, Val Accuracy: 0.9888\n",
      "--------------------------------------------------\n",
      "Epoch 6/50\n",
      "Train Loss: 0.0194, Train Accuracy: 0.9888\n",
      "Val Loss: 0.0175, Val Accuracy: 0.9882\n",
      "Saved new best model\n",
      "--------------------------------------------------\n",
      "Epoch 7/50\n",
      "Train Loss: 0.0192, Train Accuracy: 0.9888\n",
      "Val Loss: 0.0169, Val Accuracy: 0.9887\n",
      "Saved new best model\n",
      "--------------------------------------------------\n",
      "Epoch 8/50\n",
      "Train Loss: 0.0188, Train Accuracy: 0.9887\n",
      "Val Loss: 0.0171, Val Accuracy: 0.9890\n",
      "--------------------------------------------------\n",
      "Epoch 9/50\n",
      "Train Loss: 0.0181, Train Accuracy: 0.9888\n",
      "Val Loss: 0.0170, Val Accuracy: 0.9888\n",
      "--------------------------------------------------\n",
      "Epoch 10/50\n",
      "Train Loss: 0.0186, Train Accuracy: 0.9888\n",
      "Val Loss: 0.0195, Val Accuracy: 0.9879\n",
      "--------------------------------------------------\n",
      "Epoch 11/50\n",
      "Train Loss: 0.0176, Train Accuracy: 0.9892\n",
      "Val Loss: 0.0165, Val Accuracy: 0.9888\n",
      "Saved new best model\n",
      "--------------------------------------------------\n",
      "Epoch 12/50\n",
      "Train Loss: 0.0179, Train Accuracy: 0.9889\n",
      "Val Loss: 0.0168, Val Accuracy: 0.9889\n",
      "--------------------------------------------------\n",
      "Epoch 13/50\n",
      "Train Loss: 0.0177, Train Accuracy: 0.9889\n",
      "Val Loss: 0.0168, Val Accuracy: 0.9892\n",
      "--------------------------------------------------\n",
      "Epoch 14/50\n",
      "Train Loss: 0.0176, Train Accuracy: 0.9890\n",
      "Val Loss: 0.0168, Val Accuracy: 0.9888\n",
      "--------------------------------------------------\n",
      "Epoch 15/50\n",
      "Train Loss: 0.0172, Train Accuracy: 0.9889\n",
      "Val Loss: 0.0216, Val Accuracy: 0.9865\n",
      "--------------------------------------------------\n",
      "Epoch 16/50\n",
      "Train Loss: 0.0178, Train Accuracy: 0.9888\n",
      "Val Loss: 0.0171, Val Accuracy: 0.9889\n",
      "Early stopping triggered after 16 epochs\n",
      "\n",
      "Testing on easy dataset:\n",
      "Inspecting test data...\n",
      "\n",
      "Sample batch:\n",
      "Input: 28/02/2003 | Target: 02/28/2003\n",
      "Input: 1992-05-25 | Target: 05/25/1992\n",
      "Input: 28/09/1956 | Target: 09/28/1956\n",
      "Input: 13/10/1975 | Target: 10/13/1975\n",
      "Input: 11/22/1953 | Target: 11/22/1953\n",
      "Testing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 25/235 [00:08<00:53,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example conversions:\n",
      "Input: 08/29/1998 | Target: 08/29/1998 | Predicted: 896/29684\n",
      "Input: 02/19/2002 | Target: 02/19/2002 | Predicted: /029/2064\n",
      "Input: 03/26/1932 | Target: 03/26/1932 | Predicted: 30/26/1964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 50/235 [00:14<00:47,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example conversions:\n",
      "Input: 15/08/1937 | Target: 08/15/1937 | Predicted: 85/15/1974\n",
      "Input: 12/16/2069 | Target: 12/16/2069 | Predicted: /16/1/2094\n",
      "Input: 2082-09-15 | Target: 09/15/2082 | Predicted: /05/1/2082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 75/235 [00:20<00:41,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example conversions:\n",
      "Input: 01/06/2033 | Target: 06/01/2033 | Predicted: 16/06/2034\n",
      "Input: 13/02/2035 | Target: 02/13/2035 | Predicted: /023/2054\n",
      "Input: 06/05/1997 | Target: 06/05/1997 | Predicted: 56/05/1974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 100/235 [00:26<00:34,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example conversions:\n",
      "Input: 16/02/2064 | Target: 02/16/2064 | Predicted: /026/2064\n",
      "Input: 02/24/2092 | Target: 02/24/2092 | Predicted: /24/20964\n",
      "Input: 2032-12-08 | Target: 12/08/2032 | Predicted: /06/2/2032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 125/235 [00:33<00:28,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example conversions:\n",
      "Input: 06/03/2028 | Target: 06/03/2028 | Predicted: /06/09/208\n",
      "Input: 2037-09-15 | Target: 09/15/2037 | Predicted: /05/1/2037\n",
      "Input: 04/08/1915 | Target: 04/08/1915 | Predicted: 854/04952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 150/235 [00:39<00:21,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example conversions:\n",
      "Input: 2046-07-11 | Target: 07/11/2046 | Predicted: /06/1/2046\n",
      "Input: 1946-11-05 | Target: 11/05/1946 | Predicted: 16/05/1946\n",
      "Input: 1979-04-29 | Target: 04/29/1979 | Predicted: /26/2/1979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 175/235 [00:45<00:15,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example conversions:\n",
      "Input: 05/04/2035 | Target: 04/05/2035 | Predicted: 54/04/2059\n",
      "Input: 1976-03-24 | Target: 03/24/1976 | Predicted: /24/2/1976\n",
      "Input: 07/07/1992 | Target: 07/07/1992 | Predicted: 707/09/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 200/235 [00:51<00:08,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example conversions:\n",
      "Input: 10/07/1969 | Target: 10/07/1969 | Predicted: /10/19648\n",
      "Input: 03/31/2073 | Target: 03/31/2073 | Predicted: 30/31/2064\n",
      "Input: 08/11/1934 | Target: 11/08/1934 | Predicted: 16/08/1944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 225/235 [00:57<00:02,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example conversions:\n",
      "Input: 1922-03-18 | Target: 03/18/1922 | Predicted: /03/1/1922\n",
      "Input: 29/06/1986 | Target: 06/29/1986 | Predicted: 6/29/1964\n",
      "Input: 2027-01-22 | Target: 01/22/2027 | Predicted: /26/2/2027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:02<00:00,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example conversions:\n",
      "Input: 2096-08-03 | Target: 08/03/2096 | Predicted: 86/03/2096\n",
      "Input: 07/02/2055 | Target: 07/02/2055 | Predicted: /02/0/2054\n",
      "Input: 12/02/2023 | Target: 02/12/2023 | Predicted: /12/0/2034\n",
      "\n",
      "Test Accuracy: 0.1771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17706666666666668"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 24\n",
    "# Move the model to the appropriate device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "mode = \"training\" # \"training\" or \"evaluation\"\n",
    "print(f\"Mode: {mode}\")\n",
    "dataset_gen = True\n",
    "#test_dataset_generation()\n",
    "\n",
    "n = 50000\n",
    "epochs = 50\n",
    "patience = 5\n",
    "\n",
    "# # Generate the datasets \n",
    "if dataset_gen:\n",
    "    dataset, dataloaders , vocab = generate_date_dataset(n)\n",
    "    \n",
    "else:\n",
    "    print(\"Loading dataset from file\")\n",
    "    # loaded_dataset = torch.load('date_dataset.pt')\n",
    "\n",
    "    # Initialize the model\n",
    "model = DateFormatTransformer(\n",
    "    d_model=256, \n",
    "    ffn_hidden=512, \n",
    "    num_heads=8, \n",
    "    drop_prob=0.2, \n",
    "    num_layers=6,\n",
    "    max_sequence_length=24,\n",
    "    input_vocab_size=len(vocab),\n",
    "    output_vocab_size=len(vocab),\n",
    "    pad_idx=vocab['<PAD>']\n",
    ").to(device)\n",
    "\n",
    "if mode == \"training\":\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vocab['<PAD>'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience, factor=0.5)\n",
    "    # Train the model\n",
    "    train_with_curriculum(model, dataloaders, epochs, device, criterion, optimizer, patience, vocab)\n",
    "\n",
    "else:\n",
    "    model.load_state_dict(torch.load('best_model_easy.pth'))\n",
    "    model.to(device)\n",
    "\n",
    "# Test the model on 'easy' difficulty level \n",
    "for difficulty in ['easy']:\n",
    "    print(f\"\\nTesting on {difficulty} dataset:\")\n",
    "    test_dataloader = dataloaders[difficulty]['test']\n",
    "    idx_to_char = {idx: char for char, idx in vocab.items()}\n",
    "\n",
    "    print(\"Inspecting test data...\")\n",
    "    for batch in test_dataloader:\n",
    "        inputs = batch['input']\n",
    "        targets = batch['output']\n",
    "        \n",
    "        print(\"\\nSample batch:\")\n",
    "        for i in range(min(5, inputs.size(0))):\n",
    "            input_date = ''.join([idx_to_char[idx.item()] for idx in inputs[i] if idx.item() not in [vocab['<PAD>'], vocab['<START>'], vocab['<END>']]])\n",
    "            target_date = ''.join([idx_to_char[idx.item()] for idx in targets[i] if idx.item() not in [vocab['<PAD>'], vocab['<START>'], vocab['<END>']]])\n",
    "            print(f\"Input: {input_date} | Target: {target_date}\")\n",
    "        \n",
    "        # Only process one batch\n",
    "        break\n",
    "    \n",
    "test_model(model, test_dataloader, device, vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
